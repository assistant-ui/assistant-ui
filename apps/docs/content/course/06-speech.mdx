---
title: Speech
description: Implement speech-to-text and text-to-speech for voice-enabled AI conversations
---

# Speech

Speech capabilities transform your AI chat interface into a natural, voice-enabled conversation experience. Assistant UI provides built-in support for both speech-to-text (STT) and text-to-speech (TTS).

## Basic Speech Setup

Enable speech with a custom speech adapter:

```tsx
import { Thread, Composer, AssistantRuntimeProvider, useLocalRuntime } from "@assistant-ui/react";
import { WebSpeechSynthesisAdapter } from "@assistant-ui/react";

function VoiceEnabledChat() {
  const runtime = useLocalRuntime({
    adapters: {
      speech: new WebSpeechSynthesisAdapter(),
    },
  });

  return (
    <AssistantRuntimeProvider runtime={runtime}>
      <Thread>
        <Thread.Messages />
        <Composer>
          <Composer.Input />
          <Composer.Send />
        </Composer>
      </Thread>
    </AssistantRuntimeProvider>
  );
}
```

## Custom Speech Button

Create a custom speech interface using Web Speech API:

```tsx
import { useState, useRef } from "react";

function CustomSpeechButton() {
  const [isRecording, setIsRecording] = useState(false);
  const recognitionRef = useRef<SpeechRecognition | null>(null);
  
  const startRecording = () => {
    if ('webkitSpeechRecognition' in window || 'SpeechRecognition' in window) {
      const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
      const recognition = new SpeechRecognition();
      
      recognition.continuous = true;
      recognition.interimResults = true;
      recognition.lang = 'en-US';
      
      recognition.onstart = () => setIsRecording(true);
      recognition.onend = () => setIsRecording(false);
      recognition.onerror = () => setIsRecording(false);
      
      recognition.onresult = (event) => {
        const transcript = Array.from(event.results)
          .map(result => result[0].transcript)
          .join('');
        
        // Handle the transcript result
        console.log('Transcript:', transcript);
      };
      
      recognitionRef.current = recognition;
      recognition.start();
    }
  };
  
  const stopRecording = () => {
    if (recognitionRef.current) {
      recognitionRef.current.stop();
    }
  };
  
  return (
    <button
      onClick={isRecording ? stopRecording : startRecording}
      className={`p-3 rounded-full transition-all ${
        isRecording 
          ? 'bg-red-500 text-white animate-pulse' 
          : 'bg-gray-200 hover:bg-gray-300'
      }`}
    >
      {isRecording ? 'ðŸ›‘' : 'ðŸŽ¤'}
    </button>
  );
}
```

## Speech-to-Text Configuration

Configure speech recognition with custom adapters:

```tsx
import { useLocalRuntime, AssistantRuntimeProvider } from "@assistant-ui/react";
import { SpeechRecognitionAdapter } from "@assistant-ui/react";

class CustomSpeechRecognitionAdapter implements SpeechRecognitionAdapter {
  listen(): SpeechRecognitionAdapter.Session {
    let recognition: SpeechRecognition | null = null;
    const subscribers = new Set<() => void>();
    
    const session: SpeechRecognitionAdapter.Session = {
      status: { type: "starting" },
      
      async stop() {
        if (recognition) {
          recognition.stop();
        }
      },
      
      cancel() {
        if (recognition) {
          recognition.abort();
        }
      },
      
      onSpeechStart: (callback) => {
        subscribers.add(callback);
        return () => subscribers.delete(callback);
      },
      
      onSpeechEnd: (callback) => {
        // Implementation for speech end handling
        return () => {};
      },
      
      onSpeech: (callback) => {
        // Implementation for speech result handling
        return () => {};
      }
    };
    
    if ('webkitSpeechRecognition' in window || 'SpeechRecognition' in window) {
      const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
      recognition = new SpeechRecognition();
      
      recognition.continuous = true;
      recognition.interimResults = true;
      recognition.lang = 'en-US';
      
      recognition.onstart = () => {
        session.status = { type: "running" };
        subscribers.forEach(callback => callback());
      };
      
      recognition.onend = () => {
        session.status = { type: "ended", reason: "stopped" };
      };
      
      recognition.start();
    }
    
    return session;
  }
}

function ChatProvider({ children }: { children: React.ReactNode }) {
  const runtime = useLocalRuntime({
    adapters: {
      speechRecognition: new CustomSpeechRecognitionAdapter(),
    }
  });
  
  return (
    <AssistantRuntimeProvider runtime={runtime}>
      {children}
    </AssistantRuntimeProvider>
  );
}
```

## Advanced Speech Controls

Build a comprehensive speech interface with audio level monitoring:

```tsx
import { useState, useRef, useEffect } from "react";

function AdvancedSpeechControls() {
  const [isRecording, setIsRecording] = useState(false);
  const [audioLevel, setAudioLevel] = useState(0);
  const recognitionRef = useRef<SpeechRecognition | null>(null);
  const analyserRef = useRef<AnalyserNode | null>(null);
  const streamRef = useRef<MediaStream | null>(null);
  
  const startRecording = async () => {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      streamRef.current = stream;
      
      // Set up audio level monitoring
      const audioContext = new AudioContext();
      const source = audioContext.createMediaStreamSource(stream);
      const analyser = audioContext.createAnalyser();
      
      source.connect(analyser);
      analyser.fftSize = 256;
      analyserRef.current = analyser;
      
      // Start speech recognition
      if ('webkitSpeechRecognition' in window || 'SpeechRecognition' in window) {
        const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
        const recognition = new SpeechRecognition();
        
        recognition.continuous = true;
        recognition.interimResults = true;
        recognition.lang = 'en-US';
        
        recognition.onstart = () => setIsRecording(true);
        recognition.onend = () => setIsRecording(false);
        recognition.onerror = () => setIsRecording(false);
        
        recognitionRef.current = recognition;
        recognition.start();
      }
      
      // Monitor audio levels
      monitorAudio();
      
    } catch (error) {
      console.error('Failed to start recording:', error);
    }
  };
  
  const monitorAudio = () => {
    if (!analyserRef.current) return;
    
    const dataArray = new Uint8Array(analyserRef.current.frequencyBinCount);
    analyserRef.current.getByteFrequencyData(dataArray);
    
    const average = dataArray.reduce((sum, value) => sum + value, 0) / dataArray.length;
    setAudioLevel(average / 255);
    
    if (isRecording) {
      requestAnimationFrame(monitorAudio);
    }
  };
  
  const stopRecording = () => {
    if (recognitionRef.current) {
      recognitionRef.current.stop();
    }
    if (streamRef.current) {
      streamRef.current.getTracks().forEach(track => track.stop());
    }
    setAudioLevel(0);
    setIsRecording(false);
  };
  
  useEffect(() => {
    return () => {
      stopRecording();
    };
  }, []);
  
  return (
    <div className="flex items-center gap-2">
      {/* Recording Button */}
      <button
        onMouseDown={startRecording}
        onMouseUp={stopRecording}
        onMouseLeave={stopRecording}
        className={`relative p-4 rounded-full transition-all ${
          isRecording
            ? 'bg-red-500 text-white'
            : 'bg-blue-500 text-white hover:bg-blue-600'
        }`}
      >
        {isRecording ? 'ðŸ›‘' : 'ðŸŽ¤'}
        
        {/* Audio Level Indicator */}
        {isRecording && (
          <div 
            className="absolute inset-0 rounded-full border-4 border-white/30"
            style={{
              transform: `scale(${1 + audioLevel * 0.5})`,
              opacity: audioLevel
            }}
          />
        )}
      </button>
      
      {/* Status */}
      <div className="text-sm text-gray-600">
        {isRecording && (
          <span className="flex items-center gap-1">
            <div className="w-2 h-2 bg-red-500 rounded-full animate-pulse" />
            Recording...
          </span>
        )}
      </div>
    </div>
  );
}
```

## Text-to-Speech Implementation

Add voice output for assistant messages using the speech adapter:

```tsx
import { useState, useEffect } from "react";
import { MessagePrimitive, ActionBarPrimitive, useMessage, useAssistantRuntime } from "@assistant-ui/react";
import { WebSpeechSynthesisAdapter } from "@assistant-ui/react";

function SpeakableMessage() {
  const message = useMessage();
  const runtime = useAssistantRuntime();
  const [speechAdapter] = useState(() => new WebSpeechSynthesisAdapter());
  const [currentUtterance, setCurrentUtterance] = useState<any>(null);
  
  const speakMessage = async (text: string) => {
    if (currentUtterance) {
      currentUtterance.cancel();
    }
    
    const utterance = speechAdapter.speak(text);
    setCurrentUtterance(utterance);
    
    // Subscribe to status changes
    utterance.subscribe(() => {
      if (utterance.status.type === 'ended') {
        setCurrentUtterance(null);
      }
    });
  };
  
  const stopSpeaking = () => {
    if (currentUtterance) {
      currentUtterance.cancel();
      setCurrentUtterance(null);
    }
  };
  
  const isSpeaking = currentUtterance?.status.type === 'running';
  
  return (
    <MessagePrimitive.Root>
      <MessagePrimitive.Content />
      
      <MessagePrimitive.If by="assistant">
        <ActionBarPrimitive.Root>
          <button
            onClick={() => {
              if (isSpeaking) {
                stopSpeaking();
              } else {
                const textContent = message.content
                  .filter(part => part.type === 'text')
                  .map(part => part.text)
                  .join(' ');
                if (textContent) speakMessage(textContent);
              }
            }}
            className={`p-2 rounded ${
              isSpeaking ? 'bg-red-100 text-red-600' : 'bg-gray-100 hover:bg-gray-200'
            }`}
          >
            {isSpeaking ? 'ðŸ”‡' : 'ðŸ”Š'}
          </button>
          
          {/* Use the built-in stop speaking component */}
          <ActionBarPrimitive.StopSpeaking className="p-2 rounded bg-red-100 text-red-600 hover:bg-red-200">
            ðŸ›‘
          </ActionBarPrimitive.StopSpeaking>
        </ActionBarPrimitive.Root>
      </MessagePrimitive.If>
    </MessagePrimitive.Root>
  );
}

async function customTextToSpeech(text: string) {
  const response = await fetch('/api/text-to-speech', {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({ text })
  });
  
  const audioBlob = await response.blob();
  const audioUrl = URL.createObjectURL(audioBlob);
  const audio = new Audio(audioUrl);
  
  await audio.play();
}
```

## Voice Activity Detection

Implement automatic speech detection:

```tsx
import { useEffect, useRef, useState } from "react";

function VoiceActivityDetection({ onSpeechStart, onSpeechEnd }: {
  onSpeechStart: () => void;
  onSpeechEnd: () => void;
}) {
  const [isListening, setIsListening] = useState(false);
  const audioContextRef = useRef<AudioContext>();
  const analyserRef = useRef<AnalyserNode>();
  const silenceTimeoutRef = useRef<NodeJS.Timeout>();
  
  useEffect(() => {
    let stream: MediaStream;
    
    const startVAD = async () => {
      try {
        stream = await navigator.mediaDevices.getUserMedia({ audio: true });
        
        const audioContext = new AudioContext();
        const source = audioContext.createMediaStreamSource(stream);
        const analyser = audioContext.createAnalyser();
        
        source.connect(analyser);
        analyser.fftSize = 256;
        analyser.smoothingTimeConstant = 0.3;
        
        audioContextRef.current = audioContext;
        analyserRef.current = analyser;
        
        detectVoiceActivity();
      } catch (error) {
        console.error('Failed to access microphone:', error);
      }
    };
    
    const detectVoiceActivity = () => {
      if (!analyserRef.current) return;
      
      const dataArray = new Uint8Array(analyserRef.current.frequencyBinCount);
      analyserRef.current.getByteFrequencyData(dataArray);
      
      // Calculate volume level
      const volume = dataArray.reduce((sum, value) => sum + value, 0) / dataArray.length;
      const threshold = 30; // Adjust based on environment
      
      if (volume > threshold) {
        // Speech detected
        if (!isListening) {
          setIsListening(true);
          onSpeechStart();
        }
        
        // Reset silence timeout
        if (silenceTimeoutRef.current) {
          clearTimeout(silenceTimeoutRef.current);
        }
        
        silenceTimeoutRef.current = setTimeout(() => {
          setIsListening(false);
          onSpeechEnd();
        }, 1500); // 1.5 seconds of silence
      }
      
      requestAnimationFrame(detectVoiceActivity);
    };
    
    startVAD();
    
    return () => {
      if (stream) {
        stream.getTracks().forEach(track => track.stop());
      }
      if (audioContextRef.current) {
        audioContextRef.current.close();
      }
      if (silenceTimeoutRef.current) {
        clearTimeout(silenceTimeoutRef.current);
      }
    };
  }, [isListening, onSpeechStart, onSpeechEnd]);
  
  return (
    <div className={`w-3 h-3 rounded-full ${
      isListening ? 'bg-green-500 animate-pulse' : 'bg-gray-300'
    }`} />
  );
}
```

## Backend Speech Processing

Handle speech processing on the server:

```tsx
// API route for speech-to-text
export async function POST(request: Request) {
  const formData = await request.formData();
  const audioFile = formData.get('audio') as File;
  
  if (!audioFile) {
    return new Response('No audio file provided', { status: 400 });
  }
  
  try {
    // Using OpenAI Whisper
    const transcription = await openai.audio.transcriptions.create({
      file: audioFile,
      model: 'whisper-1',
      language: 'en',
      response_format: 'json'
    });
    
    return Response.json({ text: transcription.text });
  } catch (error) {
    console.error('Speech recognition failed:', error);
    return new Response('Speech recognition failed', { status: 500 });
  }
}

// API route for text-to-speech
export async function POST(request: Request) {
  const { text } = await request.json();
  
  if (!text) {
    return new Response('No text provided', { status: 400 });
  }
  
  try {
    // Using OpenAI TTS
    const mp3 = await openai.audio.speech.create({
      model: 'tts-1',
      voice: 'alloy',
      input: text,
      response_format: 'mp3'
    });
    
    const buffer = Buffer.from(await mp3.arrayBuffer());
    
    return new Response(buffer, {
      headers: {
        'Content-Type': 'audio/mpeg',
        'Content-Length': buffer.length.toString()
      }
    });
  } catch (error) {
    console.error('Text-to-speech failed:', error);
    return new Response('Text-to-speech failed', { status: 500 });
  }
}
```

## Accessibility Considerations

Ensure speech features are accessible:

```tsx
import { useState } from "react";

function AccessibleSpeechButton() {
  const [isRecording, setIsRecording] = useState(false);
  
  return (
    <button
      aria-label={isRecording ? 'Stop recording' : 'Start recording'}
      aria-pressed={isRecording}
      className="p-3 rounded-full focus:outline-none focus:ring-2 focus:ring-blue-500"
    >
      <span className="sr-only">
        {isRecording ? 'Recording in progress' : 'Click to start recording'}
      </span>
      {isRecording ? 'ðŸ›‘' : 'ðŸŽ¤'}
    </button>
  );
}
```

## Best Practices

1. **Provide visual feedback** for recording state
2. **Handle permissions gracefully** with clear error messages
3. **Use noise cancellation** when available
4. **Implement timeout handling** for long recordings
5. **Support keyboard shortcuts** for accessibility
6. **Provide fallback options** when speech is unavailable

## Exercise: Voice Chat

Build a complete voice chat interface that includes:
- Push-to-talk recording
- Automatic speech detection
- Real-time audio level visualization
- Text-to-speech for responses
- Offline speech processing fallback

## Next Steps

Speech capabilities make your AI chat more natural and accessible. Next, let's explore how to implement **Tools and Functions** for interactive AI capabilities.

Continue to [Tools and Functions â†’](./07-tools-functions)
